# Model Leaderboard Configuration
# Run with: python -m itbench_leaderboard --config model_leaderboard.toml

# ============================================================================
# Scenarios Configuration
# ============================================================================

# Path to the ITBench scenarios directory
# Can point to:
#   - A directory containing Scenario-* subdirs
#   - A directory containing numbered subdirs (1/, 2/, 3/) with ground_truth.yaml
#   - A nested structure like ./snapshots/sre/version/uuid/
scenarios_dir = "./ITBench-Lite"

# Output directory for leaderboard results
# Structure: leaderboard_dir/{prompt}_{model}_{commit}/{scenario}/{run_id}/
leaderboard_dir = "./leaderboard_results"

# ============================================================================
# Run Configuration
# ============================================================================

# Number of runs per scenario (for statistical significance)
runs_per_scenario = 1

# Concurrency settings
# When concurrent=true, ALL experiments (scenarios × runs) run in parallel
# up to max_workers limit. Agents are still processed sequentially to see trends.
# Consider API rate limits when setting max_workers (typically 10-20 max).
concurrent = true
max_workers = 10

# Trace collection (OTEL)
collect_traces = true

# ============================================================================
# Judge Configuration
# ============================================================================
# The judge evaluates agent outputs against ground truth.
# Routed through local LiteLLM proxy for unified configuration

[judge]
model = "gemini-2.5-pro"
provider = "litellm_proxy"
base_url = "http://localhost:4000"

# ============================================================================
# Agent Configurations
# ============================================================================
# Add agents to compare. Each agent runs on all scenarios.
#
# A run is uniquely identified by the combination of:
#   1. prompt   - prompt identifier (e.g., "react_shell_investigation")
#   2. commit   - git commit ID (auto-captured at runtime if not specified)
#   3. model    - model name
#
# Results are stored at: leaderboard_dir/{prompt}_{model}_{commit}/{scenario}/
#
# ============================================================================
# LiteLLM Proxy Configuration
# ============================================================================
# All agents AND judge use local LiteLLM proxy (http://localhost:4000)
# The proxy routes to various upstream providers (OpenRouter, OpenAI, etc.)
#
# Start the proxy with: litellm --config litellm_config.yaml
#
# Required environment variables (for proxy operation):
#   - OPENROUTER_API_KEY: For models routed through OpenRouter
#   - OPENAI_API_KEY: For direct OpenAI models
#
# The connection test validates the entire chain: proxy → upstream APIs → inference

# ============================================================================
# Agent 1: GPT-5.2 (via LiteLLM proxy)
# ============================================================================
[[agents]]
prompt = "react_shell_investigation"
commit = ""                                       # Auto-capture HEAD
model = "gpt-5.2-direct"                          # Model name in LiteLLM proxy
provider = "litellm"                              # Uses bundled litellm config
prompt_file = "./zero/zero-config/prompts/react_shell_investigation.md"
wire_api = "responses"                            # OpenAI-compatible models
user_query = "You must use the following application topology file is located at metadata/otel_demo_astronomy_shop/architecture.json. We want to explain alerts in otel-demo namespace only. Lets begin the investigation."

# ============================================================================
# Agent 2: Gemini 2.5 Pro (via LiteLLM proxy)
# ============================================================================
[[agents]]
prompt = "react_shell_investigation"
commit = ""
model = "gemini-2.5-pro"                          # Model name in LiteLLM proxy
provider = "litellm"                              # Uses bundled litellm config
prompt_file = "./zero/zero-config/prompts/react_shell_investigation.md"
wire_api = "chat"                                 # Gemini requires chat format
user_query = "You must use the following application topology file is located at metadata/otel_demo_astronomy_shop/architecture.json. We want to explain alerts in otel-demo namespace only. Lets begin the investigation."

# ============================================================================
# Additional Agent Examples (commented out)
# ============================================================================
# Add more models from your LiteLLM proxy by uncommenting and adjusting:
#
# [[agents]]
# prompt = "react_shell_investigation"
# model = "gpt-5.2"                              # Another model from proxy
# provider = "litellm"
# prompt_file = "./zero/zero-config/prompts/react_shell_investigation.md"
# wire_api = "responses"
# user_query = "You must use the following application topology file is located at metadata/otel_demo_astronomy_shop/architecture.json. We want to explain alerts in otel-demo namespace only. Lets begin the investigation."
#
# [[agents]]
# prompt = "react_shell_investigation"
# model = "claude-4-5-opus-latest"               # Claude from proxy
# provider = "litellm"
# prompt_file = "./zero/zero-config/prompts/react_shell_investigation.md"
# wire_api = "chat"                              # Claude uses chat format
# user_query = "You must use the following application topology file is located at metadata/otel_demo_astronomy_shop/architecture.json. We want to explain alerts in otel-demo namespace only. Lets begin the investigation."