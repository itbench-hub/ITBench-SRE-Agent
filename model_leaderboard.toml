# Model Leaderboard Configuration
# Run with: python -m itbench_leaderboard --config model_leaderboard.toml

# ============================================================================
# Scenarios Configuration
# ============================================================================

# Path to the ITBench scenarios directory
# Can point to:
#   - A directory containing Scenario-* subdirs
#   - A directory containing numbered subdirs (1/, 2/, 3/) with ground_truth.yaml
#   - A nested structure like ./snapshots/sre/version/uuid/
scenarios_dir = "./ITBench-Snapshots"

# Output directory for leaderboard results
# Structure: leaderboard_dir/{prompt}_{model}_{commit}/{scenario}/{run_id}/
leaderboard_dir = "./leaderboard_results"

# ============================================================================
# Run Configuration
# ============================================================================

# Number of runs per scenario (for statistical significance)
runs_per_scenario = 3

# Concurrency settings
concurrent = true
max_workers = 3

# Trace collection (OTEL)
collect_traces = true

# ============================================================================
# Judge Configuration
# ============================================================================
# The judge evaluates agent outputs against ground truth using litellm.
# Supports all litellm providers: OpenAI, Azure, Anthropic, Google, LiteLLM proxy, etc.
#
# API key is read from environment based on provider:
#   - openrouter: OR_API_KEY or OPENROUTER_API_KEY
#   - openai: OPENAI_API_KEY
#   - azure: AZURE_OPENAI_API_KEY
#   - litellm_proxy: LITELLM_API_KEY or OPENAI_API_KEY

[judge]
model = "openrouter/google/gemini-2.5-pro"
provider = "openrouter"
base_url = "https://openrouter.ai/api/v1"

# --- Alternative Judge Configurations ---
#
# LiteLLM Proxy (self-hosted):
# [judge]
# model = "gpt-4"                          # Model name as configured in your proxy
# provider = "litellm_proxy"
# base_url = "http://localhost:4000"       # Your LiteLLM proxy URL
#
# OpenAI Direct:
# [judge]
# model = "gpt-4o"
# provider = "openai"
# base_url = "https://api.openai.com/v1"
#
# Azure OpenAI:
# [judge]
# model = "azure/gpt-4"                    # azure/<deployment_name>
# provider = "azure"
# base_url = "https://YOUR_RESOURCE.openai.azure.com"

# ============================================================================
# Agent Configurations
# ============================================================================
# Add agents to compare. Each agent runs on all scenarios.
#
# A run is uniquely identified by the combination of:
#   1. prompt   - prompt identifier (e.g., "tap", "fs_code")
#   2. commit   - git commit ID (auto-captured at runtime if not specified)
#   3. model    - model name
#
# Results are stored at: leaderboard_dir/{prompt}_{model}_{commit}/{scenario}/
#
# Required environment variables:
#   - OR_API_KEY: OpenRouter API key
#   - ETE_API_KEY: ETE LiteLLM Proxy API key
#   - AZURE_OPENAI_API_KEY: Azure OpenAI API key

[[agents]]
# --- Run Identifier (prompt + commit + model) ---
prompt = "tap"                                    # Prompt identifier (e.g., "tap", "fs_code")
model = "Azure/gpt-5.1-2025-11-13"                # Model name
commit = ""                                       # Git commit ID (leave empty to auto-capture HEAD)

# --- Model Configuration ---
provider = "ete"
prompt_file = "./zero/zero-config/prompts/tap.md"
# wire_api: "chat" (default, works with most models) or "responses" (OpenAI models only)
wire_api = "responses"
# Optional: extra variables for prompt substitution (in addition to SNAPSHOT_DIRS)
# [agents.variables]
# MY_VAR = "value"
# Optional: user query to append to agent run
user_query = ""

# Uncomment to add more agents for comparison:

# [[agents]]
# prompt = "tap"
# model = "google/gemini-2.5-pro"
# commit = ""
# provider = "openrouter"
# prompt_file = "./zero/zero-config/prompts/tap.md"
# wire_api = "chat"  # Gemini uses chat format
# user_query = ""

# [[agents]]
# prompt = "fs_code"
# model = "anthropic/claude-opus-4"
# commit = ""
# provider = "openrouter"
# prompt_file = "./zero/zero-config/prompts/fs_code.md"
# wire_api = "chat"  # Claude uses chat format

# [[agents]]
# prompt = "tap"
# model = "openai/o4-mini"
# commit = ""
# provider = "openrouter"
# prompt_file = "./zero/zero-config/prompts/tap.md"
# wire_api = "responses"  # OpenAI models can use responses format
