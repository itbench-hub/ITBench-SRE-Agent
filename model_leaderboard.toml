# Model Leaderboard Configuration
# Run with: python -m itbench_leaderboard --config model_leaderboard.toml

# ============================================================================
# Scenarios Configuration
# ============================================================================

# Path to the ITBench scenarios directory
# Can point to:
#   - A directory containing Scenario-* subdirs
#   - A directory containing numbered subdirs (1/, 2/, 3/) with ground_truth.yaml
#   - A nested structure like ./snapshots/sre/version/uuid/
scenarios_dir = "./ITBench-Snapshots"

# Output directory for leaderboard results
# Structure: leaderboard_dir/{prompt}_{model}_{commit}/{scenario}/{run_id}/
leaderboard_dir = "./leaderboard_results"

# ============================================================================
# Run Configuration
# ============================================================================

# Number of runs per scenario (for statistical significance)
runs_per_scenario = 1

# Concurrency settings
# When concurrent=true, ALL experiments (scenarios Ã— runs) run in parallel
# up to max_workers limit. Agents are still processed sequentially to see trends.
# Consider API rate limits when setting max_workers (typically 10-20 max).
concurrent = true
max_workers = 10

# Trace collection (OTEL)
collect_traces = true

# ============================================================================
# Judge Configuration
# ============================================================================
# The judge evaluates agent outputs against ground truth using litellm.
# Supports all litellm providers: OpenAI, Azure, Anthropic, Google, LiteLLM proxy, etc.
#
# API key is read from environment based on provider:
#   - openrouter: OR_API_KEY or OPENROUTER_API_KEY
#   - openai: OPENAI_API_KEY
#   - azure: AZURE_OPENAI_API_KEY
#   - litellm_proxy: LITELLM_API_KEY or OPENAI_API_KEY

[judge]
model = "litellm_proxy/GCP/gemini-2.5-pro"
provider = "litellm_proxy"
base_url = "https://ete-litellm.ai-models.vpc-int.res.ibm.com"

# --- Alternative Judge Configurations ---
#
# LiteLLM Proxy (self-hosted):
# [judge]
# model = "gpt-4"                          # Model name as configured in your proxy
# provider = "litellm_proxy"
# base_url = "http://localhost:4000"       # Your LiteLLM proxy URL
#
# OpenAI Direct:
# [judge]
# model = "gpt-4o"
# provider = "openai"
# base_url = "https://api.openai.com/v1"
#
# Azure OpenAI:
# [judge]
# model = "azure/gpt-4"                    # azure/<deployment_name>
# provider = "azure"
# base_url = "https://YOUR_RESOURCE.openai.azure.com"

# ============================================================================
# Agent Configurations
# ============================================================================
# Add agents to compare. Each agent runs on all scenarios.
#
# A run is uniquely identified by the combination of:
#   1. prompt   - prompt identifier (e.g., "tap", "fs_code")
#   2. commit   - git commit ID (auto-captured at runtime if not specified)
#   3. model    - model name
#
# Results are stored at: leaderboard_dir/{prompt}_{model}_{commit}/{scenario}/
#
# Required environment variables:
#   - OR_API_KEY: OpenRouter API key
#   - ETE_API_KEY: ETE LiteLLM Proxy API key
#   - AZURE_OPENAI_API_KEY: Azure OpenAI API key

# [[agents]]
# # --- Run Identifier (prompt + commit + model) ---
# prompt = "react with code"                                    # Prompt identifier (e.g., "tap", "fs_code")
# commit = ""                                       # Git commit ID (leave empty to auto-capture HEAD)
# model = "Azure/gpt-5.1-2025-11-13"                # Model name

# # --- Model Configuration ---
# provider = "ete"
# prompt_file = "./zero/zero-config/prompts/fs_code.md"
# # wire_api: "chat" (default, works with most models) or "responses" (OpenAI models only)
# wire_api = "responses"
# # Optional: extra variables for prompt substitution (in addition to SNAPSHOT_DIRS)
# # [agents.variables]
# # MY_VAR = "value"
# # Optional: user query to append to agent run
# user_query = "You must use the following application topology file is located at /Users/saurabhjha/projects/open_source/sre_support_agent/datasets/application_code/otel_demo_application_architecture.json. Lets begin the investigation."

# # Uncomment to add more agents for comparison:

 [[agents]]
 prompt = "react with code"  
 name = "gemini-2.5-pro"
 model = "google/gemini-2.5-pro"
 provider = "openrouter"
 commit = ""
 prompt_file = "./zero/zero-config/prompts/fs_code.md"
 wire_api = "chat"  # Gemini uses chat format
 user_query = "You must use the following application topology file is located at /Users/saurabhjha/projects/open_source/sre_support_agent/datasets/application_code/otel_demo_application_architecture.json. Lets begin the investigation."


# [[agents]]
# prompt = "react with code"   
# model = "anthropic/claude-opus-4"
# commit = ""
# provider = "openrouter"
# prompt_file = "./zero/zero-config/prompts/fs_code.md"
# wire_api = "chat"  # Claude uses chat format
# user_query = "You must use the following application topology file is located at /Users/saurabhjha/projects/open_source/sre_support_agent/datasets/application_code/otel_demo_application_architecture.json. Lets begin the investigation."

# [[agents]]
# prompt = "react with code"  
# commit = ""
# model = "Azure/o4-mini"
# provider = "ete"
# prompt_file = "./zero/zero-config/prompts/fs_code.md"
# wire_api = "responses"  # OpenAI models can use responses format
# user_query = "You must use the following application topology file is located at /Users/saurabhjha/projects/open_source/sre_support_agent/datasets/application_code/otel_demo_application_architecture.json. Lets begin the investigation."